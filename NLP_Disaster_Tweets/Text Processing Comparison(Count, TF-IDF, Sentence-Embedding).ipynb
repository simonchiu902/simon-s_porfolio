{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a054eb8-cf6e-401d-b731-b084e35bcbaf",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook aims to explore the impact of various text processing techniques on prediction outcomes. Specifically, it will investigate the performance differences between methods such as word frequency analysis, TF-IDF (Term Frequency-Inverse Document Frequency), and sentence embedding in the context of making predictions.\n",
    "\n",
    "To ensure that variations primarily arise from different text processing techniques, we will exclusively employ logistic regression and Naive Bayes as our chosen classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96110f4d-821e-487a-8114-54d6d1627a35",
   "metadata": {},
   "source": [
    "### Import Basic Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dfb626-7e21-4bf9-a8f9-8f7ed08319ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50e6c8-9133-4577-91d7-ce9a38ef6c59",
   "metadata": {},
   "source": [
    "### Import Packages for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08a2fc8-62fb-40b2-bebd-07c416daf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, f1_score,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628a85c-c3ca-4cf9-985d-bbe8a9923c91",
   "metadata": {},
   "source": [
    "### Import Packages for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c73639-5c5a-4d00-b45c-05f7afefd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdce57-6d95-48fd-96ec-aac0f3ae7411",
   "metadata": {},
   "source": [
    "### Import Packages to ignore warning\n",
    "\n",
    "is_sparse function is deprecated, but it should not impact the functionality of models and results. Thus, we ignore the warning for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1baa0a8-dcb6-4ff8-a2c9-1361958e59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88f867-901c-4988-9c0a-e80a0dbff173",
   "metadata": {},
   "source": [
    "### Read Files\n",
    "\n",
    "The dataset is downloaded from https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30d1eaa-01ff-4898-8b2f-904c2e4f7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"./kaggle/input/train.csv\")\n",
    "test_df = pd.read_csv(\"./kaggle/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ba9620-ce2f-40c7-ac63-d1f775b32226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac2f65e-2afa-4d62-897f-2af161f889a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456a480-33d8-44bd-b09b-065d00c8085a",
   "metadata": {},
   "source": [
    "### Clean Text Column\n",
    "\n",
    "Text cleaning is done from a python script (Preprocessing_for_Text-Processing-Comparison.py). The following steps are involved:\n",
    "\n",
    "* For all models, the 1-5 steps are invovled:\n",
    "1. remove link\n",
    "2. remove @account\n",
    "3. remove line breaks\n",
    "4. remove # from hashtag\n",
    "5. remove non-ASCII characters\n",
    "\n",
    "* For sentence embedding, which LLM is invovled, since it's good at understanding sequencial meanings, the following steps will not be performed (We'll also validate this assumption in Method3)\n",
    "6. tokenize text\n",
    "7. change words into lower case\n",
    "8. only include alphabetic words\n",
    "9. remove stop words\n",
    "10. steeming words\n",
    "\n",
    "Upon completion of the cleaning process, two additional columns are created. The first column, named `text_clean`, consists of a list of tokenized words. The second column, denoted as `text_clean_string`, is a representation formed by combining the tokens from the text_clean column with spaces in between each token. They will be used for the three different methods later.\n",
    "\n",
    "For more details, can refer to the python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56e22e2-6346-4dd1-a083-440c95fb3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preprocessing_for_Text_Processing_Comparison as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098e8ea8-7be8-42a8-aaf8-35069dac5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pp.process_text(train_df)\n",
    "test = pp.process_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a96cc472-35da-49be-8452-4b2a39f0bc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, place, notifi, offic, evacu, shel...</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2       1  [resid, ask, place, notifi, offic, evacu, shel...   \n",
       "3       1  [peopl, receiv, wildfir, evacu, order, califor...   \n",
       "4       1  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   text_clean_string  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask place notifi offic evacu shelter pla...  \n",
       "3        peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fd723b-4fd2-4953-b18f-495c14b0ff31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[happen, terribl, car, crash]</td>\n",
       "      <td>happen terribl car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>[heard, earthquak, differ, citi, stay, safe, e...</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>[forest, fire, spot, pond, gees, flee, across,...</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>[apocalyps, light, spokan, wildfir]</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                      [happen, terribl, car, crash]   \n",
       "1  [heard, earthquak, differ, citi, stay, safe, e...   \n",
       "2  [forest, fire, spot, pond, gees, flee, across,...   \n",
       "3                [apocalyps, light, spokan, wildfir]   \n",
       "4           [typhoon, soudelor, kill, china, taiwan]   \n",
       "\n",
       "                                   text_clean_string  \n",
       "0                           happen terribl car crash  \n",
       "1      heard earthquak differ citi stay safe everyon  \n",
       "2  forest fire spot pond gees flee across street ...  \n",
       "3                     apocalyps light spokan wildfir  \n",
       "4                 typhoon soudelor kill china taiwan  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4350f3-4909-4f4d-8279-6c22236fbae9",
   "metadata": {},
   "source": [
    "### Method1: Word Frequency\n",
    "\n",
    "To kick start a base method, let's start with using the count of words in each tweet.\n",
    "Below will be using `CountVectorizer` to build the count of words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "885d8e24-7fc1-4a0c-8e54-f649109231a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ee90c51-524d-4295-baca-489edd07d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4a4a9c-c9ac-48b7-87aa-2a5db5d9c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deed reason earthquak may allah forgiv us'\n",
      " 'forest fire near la rong sask canada']\n",
      "['allah' 'canada' 'deed' 'earthquak' 'fire' 'forest' 'forgiv' 'la' 'may'\n",
      " 'near' 'reason' 'rong' 'sask' 'us']\n",
      "{'deed': 2, 'reason': 10, 'earthquak': 3, 'may': 8, 'allah': 0, 'forgiv': 6, 'us': 13, 'forest': 5, 'fire': 4, 'near': 9, 'la': 7, 'rong': 11, 'sask': 12, 'canada': 1}\n",
      "[[1 0 1 1 0 0 1 0 1 0 1 0 0 1]\n",
      " [0 1 0 0 1 1 0 1 0 1 0 1 1 0]]\n",
      "(2, 14)\n"
     ]
    }
   ],
   "source": [
    "## let's take a look at expected output by using first 2 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train[\"text_clean_string\"][0:2])\n",
    "print(train[\"text_clean_string\"][0:2].values)\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_vectorizer.vocabulary_)\n",
    "print(example_train_vectors.toarray())\n",
    "print(example_train_vectors.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49259b4-4628-460b-90ef-3f0b8b1dc0cb",
   "metadata": {},
   "source": [
    "The above tells us that, there are 14 unique words (or \"tokens\") in the first two tweets.\n",
    "\n",
    "Now let's create vectors for all of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b644b64a-29f3-4770-95af-dfeaccbc3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train[\"text_clean_string\"])\n",
    "\n",
    "# note that we're NOT using .fit_transform() here. Using just .transform() makes sure that train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test[\"text_clean_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fad47072-8736-44c8-95ca-324dbd19dc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 10532)\n"
     ]
    }
   ],
   "source": [
    "# print(count_vectorizer.get_feature_names_out())\n",
    "# print(count_vectorizer.vocabulary_)\n",
    "# print(example_train_vectors.toarray())\n",
    "print(train_vectors.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f0c7d-ed8c-4683-a9fe-68b04bbcc377",
   "metadata": {},
   "source": [
    "1-1 Build model without standerdize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caaacc58-c042-42b6-8532-7800244d9478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9303433220877158\n",
      "F1 Score: 0.7733118971061093\n",
      "      0     1\n",
      "0  3395    78\n",
      "1   273  2344\n",
      "     0    1\n",
      "0  760  109\n",
      "1  173  481\n"
     ]
    }
   ],
   "source": [
    "X = train_vectors\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_train, LR.predict(X_train)))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, LR.predict(X_train))))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "443d9dfc-f009-4f3f-a0ef-0b7834748ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiroshima: 2.645388489888987\n",
      "wildfir: 2.3347055956738036\n",
      "earthquak: 2.0389962276401725\n",
      "typhoon: 1.910274615457201\n",
      "storm: 1.902511518781684\n",
      "debri: 1.795257627619148\n",
      "tornado: 1.7497896583076402\n",
      "spill: 1.7224985363487062\n",
      "massacr: 1.6862271855273108\n",
      "migrant: 1.5787320214120713\n",
      "----------------------------------------\n",
      "nowplay: -1.2654459304583408\n",
      "love: -1.2380933407793044\n",
      "ticket: -1.2177880729937431\n",
      "never: -1.2001342147749514\n",
      "upheav: -1.194503451145276\n",
      "write: -1.1776942612336967\n",
      "lmao: -1.1301736722566142\n",
      "technolog: -1.0972574486053386\n",
      "let: -1.0860564440658924\n",
      "career: -1.0761327710359425\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words) from the CountVectorizer\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefficients = LR.coef_[0]\n",
    "\n",
    "# Create a dictionary that associates feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the features by their coefficients to find the most important positive coeficient ones\n",
    "sorted_features_positive = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort the features by their coefficients to find the most important negative coeficient ones\n",
    "sorted_features_negative = sorted(feature_coefficients.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the top N most important positive features\n",
    "top_n = 10  # Change this value to see more or fewer features\n",
    "for feature, coefficient in sorted_features_positive[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "print('----------------------------------------')\n",
    "    \n",
    "# Print the top N most important negative features\n",
    "for feature, coefficient in sorted_features_negative[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88823f92-5f17-43bb-82ab-4c95269d8237",
   "metadata": {},
   "source": [
    "1-2 Build model with MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21590015-02df-4d23-a26f-901f2f2c343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9252055343894124\n",
      "F1 Score: 0.761437908496732\n",
      "      0     1\n",
      "0  3410    63\n",
      "1   310  2307\n",
      "     0    1\n",
      "0  765  104\n",
      "1  188  466\n"
     ]
    }
   ],
   "source": [
    "#use scalor\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_vectors)\n",
    "X_test_scaled = scaler.transform(test_vectors)\n",
    "\n",
    "\n",
    "X = X_train_scaled\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_train, LR.predict(X_train)))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, LR.predict(X_train))))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430ed870-e965-4ec1-b128-ded88e64a0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire: 3.097592025002737\n",
      "hiroshima: 3.030802588543393\n",
      "storm: 2.564572486204606\n",
      "wildfir: 2.5163620520627994\n",
      "earthquak: 2.172251793362818\n",
      "evacu: 2.1564213525966407\n",
      "california: 2.147375307697853\n",
      "murder: 2.075147318262127\n",
      "kill: 2.071626929371796\n",
      "forest: 2.0433387973774666\n",
      "----------------------------------------\n",
      "love: -1.9045865156101898\n",
      "upheav: -1.4344258146514115\n",
      "make: -1.4066990584691839\n",
      "let: -1.3786792371575622\n",
      "never: -1.3008434730429905\n",
      "nowplay: -1.2518685391618602\n",
      "fuck: -1.2097839463139448\n",
      "poll: -1.2017224793245656\n",
      "lmao: -1.178034252929234\n",
      "career: -1.1620973133203012\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words) from the CountVectorizer\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefficients = LR.coef_[0]\n",
    "\n",
    "# Create a dictionary that associates feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the features by their coefficients to find the most important positive coeficient ones\n",
    "sorted_features_positive = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort the features by their coefficients to find the most important negative coeficient ones\n",
    "sorted_features_negative = sorted(feature_coefficients.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the top N most important positive features\n",
    "top_n = 10  # Change this value to see more or fewer features\n",
    "for feature, coefficient in sorted_features_positive[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "print('----------------------------------------')\n",
    "    \n",
    "# Print the top N most important negative features\n",
    "for feature, coefficient in sorted_features_negative[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e705c47f-f952-4f48-8499-c05eb1939312",
   "metadata": {},
   "source": [
    "Conclusion: without MaxAbsScaler, the F1 score for both training and testing are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839b2d6-760e-4f56-8ab7-58bb749eacf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3846346-6905-4a03-b634-03e251d4bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.8802537668517051\n",
      "Test F1 Score: 0.7637795275590552\n",
      "      0     1\n",
      "0  3266   207\n",
      "1   397  2220\n",
      "     0    1\n",
      "0  738  131\n",
      "1  169  485\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Multinomial Naive Bayes model\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "train_predicted = nb_classifier.predict(X_train)\n",
    "test_predicted = nb_classifier.predict(X_test)\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5cd37b-a01b-4d0c-af07-87c76bee04d6",
   "metadata": {},
   "source": [
    "Conclusion: Naive Bayes(0.76) performs similar like Logistic Regression(0.77)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b022fe-ede0-4946-916c-d6afe936dca8",
   "metadata": {},
   "source": [
    "### Method2: TF-IDF\n",
    "\n",
    "Now, let's try TF-IDF by using `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ea6ab7a-2aa0-4db8-a810-82ca151b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67f0b7e2-c3bd-4086-a21f-34fff2ce2d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 1520)\n"
     ]
    }
   ],
   "source": [
    "# Only include >=10 occurrences\n",
    "# Have unigrams and bigrams\n",
    "vec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n",
    "\n",
    "text_vec = vec_text.fit_transform(train['text_clean_string'])\n",
    "text_vec_test = vec_text.transform(test['text_clean_string'])\n",
    "X_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names_out())\n",
    "X_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names_out())\n",
    "print (X_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3020362b-8c92-4d3b-9a88-c3e2608dac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablaz</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accid</th>\n",
       "      <th>accord</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth save</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282098</td>\n",
       "      <td>0.295891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 1520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aba  aba woman  abandon       abc  abc news  abl  ablaz  absolut  accid  \\\n",
       "0     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "1     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "2     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "3     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "4     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "...   ...        ...      ...       ...       ...  ...    ...      ...    ...   \n",
       "7608  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7609  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7610  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7611  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7612  0.0        0.0      0.0  0.282098  0.295891  0.0    0.0      0.0    0.0   \n",
       "\n",
       "      accord  ...  yesterday   yo  york  young  youth  youth save  youtub  \\\n",
       "0        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "1        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "2        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "3        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "4        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "...      ...  ...        ...  ...   ...    ...    ...         ...     ...   \n",
       "7608     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7609     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7610     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7611     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7612     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "\n",
       "       yr  yyc  zone  \n",
       "0     0.0  0.0   0.0  \n",
       "1     0.0  0.0   0.0  \n",
       "2     0.0  0.0   0.0  \n",
       "3     0.0  0.0   0.0  \n",
       "4     0.0  0.0   0.0  \n",
       "...   ...  ...   ...  \n",
       "7608  0.0  0.0   0.0  \n",
       "7609  0.0  0.0   0.0  \n",
       "7610  0.0  0.0   0.0  \n",
       "7611  0.0  0.0   0.0  \n",
       "7612  0.0  0.0   0.0  \n",
       "\n",
       "[7613 rows x 1520 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cdc659e-5e11-48f9-b5cb-eabfb0d005ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7975993377483444\n",
      "F1 Score: 0.7660626029654036\n",
      "      0     1\n",
      "0  3185   288\n",
      "1   690  1927\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>774</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  774   95\n",
       "1  189  465"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train_text\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_train, LR.predict(X_train)))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, LR.predict(X_train))))\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "992ca6b9-e820-43de-bba2-1792308c03d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiroshima: 3.6523858852884152\n",
      "wildfir: 3.204129362061818\n",
      "california: 2.757571778025467\n",
      "kill: 2.7200358240055658\n",
      "forest: 2.5683849832564016\n",
      "storm: 2.519988696677238\n",
      "earthquak: 2.360874176144914\n",
      "evacu: 2.2848403258261873\n",
      "debri: 2.2419491470055353\n",
      "flood: 2.176170826767648\n",
      "----------------------------------------\n",
      "love: -2.2125034179401326\n",
      "let: -1.8439255378635953\n",
      "bag: -1.7823108277022437\n",
      "want: -1.7600085185610215\n",
      "fuck: -1.736474276515424\n",
      "new: -1.6127636473486138\n",
      "make: -1.560242948545216\n",
      "blew: -1.52134944892504\n",
      "harm: -1.5080448559621575\n",
      "upheav: -1.4948783135997972\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words) from the CountVectorizer\n",
    "feature_names = vec_text.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefficients = LR.coef_[0]\n",
    "\n",
    "# Create a dictionary that associates feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the features by their coefficients to find the most important positive coeficient ones\n",
    "sorted_features_positive = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort the features by their coefficients to find the most important negative coeficient ones\n",
    "sorted_features_negative = sorted(feature_coefficients.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the top N most important positive features\n",
    "top_n = 10  # Change this value to see more or fewer features\n",
    "for feature, coefficient in sorted_features_positive[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "print('----------------------------------------')\n",
    "    \n",
    "# Print the top N most important negative features\n",
    "for feature, coefficient in sorted_features_negative[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e36b2a-dd4f-4e54-bad2-47176ebc6750",
   "metadata": {},
   "source": [
    "The TF-IDF result(0.766) is slightly lower than Word Frequency (0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7bd08-48f6-4e4c-b761-91f6d2520723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0707d6b4-bda2-4e7f-b294-d84f62451a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.7648314127187367\n",
      "Test F1 Score: 0.7508474576271186\n",
      "      0     1\n",
      "0  3196   277\n",
      "1   825  1792\n",
      "     0    1\n",
      "0  786   83\n",
      "1  211  443\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Multinomial Naive Bayes model\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "train_predicted = nb_classifier.predict(X_train)\n",
    "test_predicted = nb_classifier.predict(X_test)\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee37c4-cd91-40fa-9a44-2c6e81667c5e",
   "metadata": {},
   "source": [
    "Conclusion: Naive Bayes(0.75) performs similar like Logistic Regression(0.766)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0412fc-533b-414e-8dd9-a547f19f18cf",
   "metadata": {},
   "source": [
    "### Method3: Sentence Embedding\n",
    "\n",
    "Here, we're using `SentenceTransformer` to transform our text into sentence embedding.\n",
    "\n",
    "*Reference link: https://www.youtube.com/watch?v=c7AqnswslWo\n",
    "\n",
    "An experiment is performed to determine whether it is more effective to create sentence embeddings from the original text or from text that has undergone lowercase conversion, removal of non-alphabetic characters, exclusion of stop words, and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4df70d52-1a60-4319-8961-2b24fda483a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package\n",
    "# !pip install --user sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaff51ed-5983-40ad-aa9c-651241675cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a87395-c473-4390-8e85-4fbc4be0c301",
   "metadata": {},
   "source": [
    "1. Embedding from processed text (lowercase, alphabetic, stop words, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db6996c0-83b5-4a8e-8e36-a05f1143e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SentenceTransformer to generate sentence embedding\n",
    "# can choose diff models from: https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train['embeddings'] = train['text_clean_string'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4426c7c2-88d8-4817-bb77-95005bfa0007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.7454909819639277\n",
      "Test F1 Score: 0.7570977917981073\n",
      "      0     1\n",
      "0  2960   513\n",
      "1   757  1860\n",
      "     0    1\n",
      "0  735  134\n",
      "1  174  480\n"
     ]
    }
   ],
   "source": [
    "X = train['embeddings'].to_list()\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "train_predicted = LR.predict(X_train)\n",
    "test_predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e5ed7-bf48-4dd9-9efd-6097f9813b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b18a8985-3283-4a9d-a697-58165d55a728",
   "metadata": {},
   "source": [
    "2. Embedding from original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbf24d00-eb47-4042-ad0f-2e4e369e0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_llm = pp.process_text(train_df,LLM = True)\n",
    "test_llm = pp.process_text(test_df,LLM = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cef96ee2-1cf4-4437-8b62-df8689208177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive wildfires evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...  \n",
       "1       1             Forest fire near La Ronge Sask. Canada  \n",
       "2       1  All residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive wildfires evacuation ord...  \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aae004ca-afc2-4e68-97f4-725483bd7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SentenceTransformer to generate sentence embedding\n",
    "# can choose diff models from: https://www.sbert.net/docs/pretrained_models.html\n",
    "train_llm['embeddings'] = train_llm['text_clean'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "110c7eb4-0797-4540-96bd-8fc6036a7caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[-0.0054457616, 0.064116865, 0.11215522, 0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[0.04022922, 0.03801415, -0.0064313184, 0.0246...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[0.13137569, 0.012401222, 0.06718611, 0.085462...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive wildfires evacuation ord...</td>\n",
       "      <td>[0.099470675, -0.033589236, 0.0065593175, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[-0.036732815, 0.10448628, 0.0742257, 0.089533...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  13,000 people receive wildfires evacuation ord...   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.0054457616, 0.064116865, 0.11215522, 0.033...  \n",
       "1  [0.04022922, 0.03801415, -0.0064313184, 0.0246...  \n",
       "2  [0.13137569, 0.012401222, 0.06718611, 0.085462...  \n",
       "3  [0.099470675, -0.033589236, 0.0065593175, 0.01...  \n",
       "4  [-0.036732815, 0.10448628, 0.0742257, 0.089533...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf1497ba-e72c-4332-9640-d8673a70bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.7826431543491182\n",
      "Test F1 Score: 0.8009516256938937\n",
      "      0     1\n",
      "0  3018   455\n",
      "1   642  1975\n",
      "     0    1\n",
      "0  767  102\n",
      "1  149  505\n"
     ]
    }
   ],
   "source": [
    "X = train_llm['embeddings'].to_list()\n",
    "y = train_llm['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "train_predicted = LR.predict(X_train)\n",
    "test_predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa23b8e-8829-4119-90f8-acefb28dfd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745762cf-5bc6-4b15-847d-c053d9e9e2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5f1a4c-7581-44a0-b50f-c901166dc4b0",
   "metadata": {},
   "source": [
    "If you run the code below, you will get the error: Negative values in data passed to MultinomialNB (input X)\n",
    "\n",
    "This is because MultinomialNB (Multinomial Naive Bayes) algorithm is designed for discrete data, specifically for data that represents counts or frequencies. In the context of text classification, it's commonly used to model the frequency of words in a document or a set of documents.\n",
    "\n",
    "Negative values are not meaningful in this context because you cannot have a negative count of words in a document. The Multinomial Naive Bayes algorithm works with non-negative integer values, which typically represent the frequency or count of each term (word) in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62d17eb2-e75e-44d2-b519-fb4451a28012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and train the Multinomial Naive Bayes model\n",
    "# nb_classifier = MultinomialNB()\n",
    "# nb_classifier.fit(X_train,y_train)\n",
    "# train_predicted = nb_classifier.predict(X_train)\n",
    "# test_predicted = nb_classifier.predict(X_test)\n",
    "# print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "# print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# # Confusion matrix\n",
    "# print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "# print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bdcd908d-8618-4584-9b59-4bb34a1ad674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.7850542386500602\n",
      "Test F1 Score: 0.7896440129449839\n",
      "      0     1\n",
      "0  3066   407\n",
      "1   663  1954\n",
      "     0    1\n",
      "0  775   94\n",
      "1  166  488\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# X_train and y_train are your sentence embeddings and labels\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "train_predicted = svm_classifier.predict(X_train)\n",
    "test_predicted = svm_classifier.predict(X_test)\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6357a3-730d-47db-8a6f-8449eeb33324",
   "metadata": {},
   "source": [
    "Conclusion: The sentence embedding with logistic regression(0.8) gives the best result among three methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb48d6-475a-4025-8845-4869f0fe5773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5107983-f327-4100-ae6d-16da58f693d9",
   "metadata": {},
   "source": [
    "Sentence embedding from `SentenceTransformer` also support semantic search. It returns the topN similar tweets by cosine similarity. \n",
    "\n",
    "Reference: https://medium.com/nlplanet/two-minutes-nlp-sentence-transformers-cheat-sheet-2e9865083e7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abaeb301-0f72-4446-aadb-e3fae4ce1314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Help me, there is a serious disaster\n",
      "hurrican tornado tsunami someon pleas tell hell happen nopow (Score: 0.4548)\n",
      "danger ok rest us danger (Score: 0.4378)\n",
      "Query: My life is so peaceful and happy\n",
      "live balanc life balanc fear allah hope merci love (Score: 0.4005)\n",
      "greater tragedi becom comfort life (Score: 0.3832)\n"
     ]
    }
   ],
   "source": [
    "# Queries and their embeddings\n",
    "queries = [\"Help me, there is a serious disaster\", \"My life is so peaceful and happy\"]\n",
    "queries_embeddings = model.encode(queries)\n",
    "\n",
    "# Ensure 'embeddings' column contains float32 or float64 arrays\n",
    "train['embeddings2'] = train['embeddings'].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "# Now you can convert the 'embeddings' column to a PyTorch tensor\n",
    "embeddings_tensor = torch.from_numpy(np.stack(train['embeddings2'].values))\n",
    "\n",
    "# Find the top-2 corpus documents matching each query\n",
    "hits = util.semantic_search(queries_embeddings, embeddings_tensor, top_k=2)\n",
    "\n",
    "# Print results of first query\n",
    "print(f\"Query: {queries[0]}\")\n",
    "for hit in hits[0]:\n",
    "    print(train['text_clean_string'][hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
    "# Query: What is Python?\n",
    "# Python is an interpreted high-level general-purpose programming language. (Score: 0.6759)\n",
    "# Python is dynamically-typed and garbage-collected. (Score: 0.6219)\n",
    "\n",
    "# Print results of second query\n",
    "print(f\"Query: {queries[1]}\")\n",
    "for hit in hits[1]:\n",
    "    print(train['text_clean_string'][hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d1386-73f5-4fec-b1e0-2714a5148794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db0fecd-6d85-4f38-9d43-03c2bcc4d3f5",
   "metadata": {},
   "source": [
    "### Conlcusion\n",
    "\n",
    "Final F1 Score: \n",
    "* Word-Frequency: 0.77\n",
    "* TF-IDF: 0.76\n",
    "* Sentence-Embedding: 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
