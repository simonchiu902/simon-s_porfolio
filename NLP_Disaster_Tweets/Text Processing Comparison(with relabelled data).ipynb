{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a054eb8-cf6e-401d-b731-b084e35bcbaf",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Based on our previous investigation in the `FP&FN problem` notebook, we identified instances where records were incorrectly labeled. Consequently, we have made the decision to relabel those FN records. In this notebook, we will employ the relabelled dataset to re-run the model and assess whether this process leads to an enhancement in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96110f4d-821e-487a-8114-54d6d1627a35",
   "metadata": {},
   "source": [
    "### Import Basic Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dfb626-7e21-4bf9-a8f9-8f7ed08319ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50e6c8-9133-4577-91d7-ce9a38ef6c59",
   "metadata": {},
   "source": [
    "### Import Packages for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08a2fc8-62fb-40b2-bebd-07c416daf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, f1_score,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628a85c-c3ca-4cf9-985d-bbe8a9923c91",
   "metadata": {},
   "source": [
    "### Import Packages for Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c73639-5c5a-4d00-b45c-05f7afefd515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdce57-6d95-48fd-96ec-aac0f3ae7411",
   "metadata": {},
   "source": [
    "### Import Packages to ignore warning\n",
    "\n",
    "is_sparse function is deprecated, but it should not impact the functionality of models and results. Thus, we ignore the warning for cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1baa0a8-dcb6-4ff8-a2c9-1361958e59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88f867-901c-4988-9c0a-e80a0dbff173",
   "metadata": {},
   "source": [
    "### Read Files\n",
    "\n",
    "The dataset is downloaded from https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30d1eaa-01ff-4898-8b2f-904c2e4f7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"./kaggle/input/train.csv\")\n",
    "test_df = pd.read_csv(\"./kaggle/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ba9620-ce2f-40c7-ac63-d1f775b32226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac2f65e-2afa-4d62-897f-2af161f889a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ce1c9c-89af-4f66-94cc-19a905796f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>relabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9794</td>\n",
       "      <td>trapped</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hollywood Movie About Trapped Miners Released ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4172</td>\n",
       "      <td>drown</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>I can't drown my demons they know how to swim</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2817</td>\n",
       "      <td>cyclone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@XHNews We need these plants in the pacific du...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7160</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Birmingham &amp; Bristol</td>\n",
       "      <td>It looks like a mudslide' poor thing! ?? #grea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8578</td>\n",
       "      <td>screams</td>\n",
       "      <td>Sheffield/Leeds</td>\n",
       "      <td>I agree with certain cultural appropriation th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>6702</td>\n",
       "      <td>lava</td>\n",
       "      <td>probably watching survivor</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>8739</td>\n",
       "      <td>sinking</td>\n",
       "      <td>MA</td>\n",
       "      <td>that horrible sinking feeling when youÂ‰Ã›Âªve...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>7147</td>\n",
       "      <td>mudslide</td>\n",
       "      <td>Ealing, London</td>\n",
       "      <td>It looks like a mudslide!' And #GBBO is back w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>8129</td>\n",
       "      <td>rescued</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heroes! A Springer Spaniel &amp;amp; her dog dad r...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>4706</td>\n",
       "      <td>epicentre</td>\n",
       "      <td>Africa</td>\n",
       "      <td>RT @calestous: Tanzania elephant population de...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id    keyword                    location  \\\n",
       "0    9794    trapped                         NaN   \n",
       "1    4172      drown                    Portugal   \n",
       "2    2817    cyclone                         NaN   \n",
       "3    7160   mudslide        Birmingham & Bristol   \n",
       "4    8578    screams             Sheffield/Leeds   \n",
       "..    ...        ...                         ...   \n",
       "876  6702       lava  probably watching survivor   \n",
       "877  8739    sinking                          MA   \n",
       "878  7147   mudslide              Ealing, London   \n",
       "879  8129    rescued                         NaN   \n",
       "880  4706  epicentre                      Africa   \n",
       "\n",
       "                                                  text  target  relabel  \n",
       "0    Hollywood Movie About Trapped Miners Released ...       1      0.0  \n",
       "1        I can't drown my demons they know how to swim       1      0.0  \n",
       "2    @XHNews We need these plants in the pacific du...       1      0.0  \n",
       "3    It looks like a mudslide' poor thing! ?? #grea...       1      0.0  \n",
       "4    I agree with certain cultural appropriation th...       1      0.0  \n",
       "..                                                 ...     ...      ...  \n",
       "876  The sunset looked like an erupting volcano ......       1      0.0  \n",
       "877  that horrible sinking feeling when youÂ‰Ã›Âªve...       1      0.0  \n",
       "878  It looks like a mudslide!' And #GBBO is back w...       1      0.0  \n",
       "879  Heroes! A Springer Spaniel &amp; her dog dad r...       1      0.0  \n",
       "880  RT @calestous: Tanzania elephant population de...       1      0.0  \n",
       "\n",
       "[881 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in relabelled data\n",
    "relabelled_df = pd.read_excel(\"Relabel for FN records.xlsx\")\n",
    "relabelled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2e93096-6d2b-476d-b045-f2afeced9429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    515\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the id needed to relabel\n",
    "relabelled_ids = relabelled_df[relabelled_df.relabel == 0].id.tolist()\n",
    "#There are 515 records to be replaced\n",
    "train_df[train_df['id'].isin(relabelled_ids)].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da59c68-7b23-4695-be8e-9099ea46184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the \"target\" column in train_df based on the IDs\n",
    "train_df['target_relabelled'] = train_df['id'].apply(lambda x: 0 if x in relabelled_ids else train_df[train_df['id'] == x]['target'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "809a3aa8-1e91-4c40-99ce-66fd29de599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    4342\n",
      "1    3271\n",
      "Name: count, dtype: int64\n",
      "target_relabelled\n",
      "0    4857\n",
      "1    2756\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df['target'].value_counts())\n",
    "print(train_df['target_relabelled'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3a7a95-a9be-4ff7-beda-4050d5f27ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_percentage(df, column, relabel = False):\n",
    "    total_count = len(df)\n",
    "    non_disaster_count = df[column].value_counts()[0]\n",
    "    disaster_count = df[column].value_counts()[1]\n",
    "    percentage_non_disaster = int((non_disaster_count / total_count) * 100)\n",
    "    percentage_disaster = int((disaster_count / total_count) * 100)\n",
    "    \n",
    "    if not relabel:\n",
    "        print(\"Original Non-Disaster:\", percentage_non_disaster, \"%\")\n",
    "        print(\"Original Disaster:\", percentage_disaster, \"%\")\n",
    "    else:\n",
    "        print(\"Relabelled Non-Disaster:\", percentage_non_disaster, \"%\")\n",
    "        print(\"Relabelled Disaster:\", percentage_disaster, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "686ff431-8be5-444f-a2b2-b50ab6a5fd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Non-Disaster: 57 %\n",
      "Original Disaster: 42 %\n",
      "Relabelled Non-Disaster: 63 %\n",
      "Relabelled Disaster: 36 %\n"
     ]
    }
   ],
   "source": [
    "target_percentage(train_df, 'target')\n",
    "target_percentage(train_df, 'target_relabelled', relabel = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5624b50-8448-40ea-9cdc-0d5e7fc81c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the modified train data\n",
    "file_path = \"./kaggle/input/train_relabelled.csv\"\n",
    "train_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5774fbfc-b14d-459c-af2e-270f31b72b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the modified train data \n",
    "#drop original target and rename target_relabelled to be target, so that we can use the previous code to run models\n",
    "train_df = pd.read_csv(\"./kaggle/input/train_relabelled.csv\")\n",
    "train_df = train_df.drop('target', axis=1)\n",
    "column_mapping = {'target_relabelled': 'target'}\n",
    "train_df = train_df.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456a480-33d8-44bd-b09b-065d00c8085a",
   "metadata": {},
   "source": [
    "### Clean Text Column\n",
    "\n",
    "Text cleaning is done from a python script (Preprocessing_for_Text-Processing-Comparison.py). The following steps are involved:\n",
    "\n",
    "* For all models, the 1-5 steps are invovled:\n",
    "1. remove link\n",
    "2. remove @account\n",
    "3. remove line breaks\n",
    "4. remove # from hashtag\n",
    "5. remove non-ASCII characters\n",
    "\n",
    "* For sentence embedding, which LLM is invovled, since it's good at understanding sequencial meanings, the following steps will not be performed (We'll also validate this assumption in Method3)\n",
    "6. tokenize text\n",
    "7. change words into lower case\n",
    "8. only include alphabetic words\n",
    "9. remove stop words\n",
    "10. steeming words\n",
    "\n",
    "Upon completion of the cleaning process, two additional columns are created. The first column, named `text_clean`, consists of a list of tokenized words. The second column, denoted as `text_clean_string`, is a representation formed by combining the tokens from the text_clean column with spaces in between each token. They will be used for the three different methods later.\n",
    "\n",
    "For more details, can refer to the python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56e22e2-6346-4dd1-a083-440c95fb3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preprocessing_for_Text_Processing_Comparison as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "098e8ea8-7be8-42a8-aaf8-35069dac5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pp.process_text(train_df)\n",
    "test = pp.process_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96cc472-35da-49be-8452-4b2a39f0bc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, place, notifi, offic, evacu, shel...</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2       1  [resid, ask, place, notifi, offic, evacu, shel...   \n",
       "3       1  [peopl, receiv, wildfir, evacu, order, califor...   \n",
       "4       1  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   text_clean_string  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask place notifi offic evacu shelter pla...  \n",
       "3        peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3fd723b-4fd2-4953-b18f-495c14b0ff31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[happen, terribl, car, crash]</td>\n",
       "      <td>happen terribl car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>[heard, earthquak, differ, citi, stay, safe, e...</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>[forest, fire, spot, pond, gees, flee, across,...</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>[apocalyps, light, spokan, wildfir]</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                      [happen, terribl, car, crash]   \n",
       "1  [heard, earthquak, differ, citi, stay, safe, e...   \n",
       "2  [forest, fire, spot, pond, gees, flee, across,...   \n",
       "3                [apocalyps, light, spokan, wildfir]   \n",
       "4           [typhoon, soudelor, kill, china, taiwan]   \n",
       "\n",
       "                                   text_clean_string  \n",
       "0                           happen terribl car crash  \n",
       "1      heard earthquak differ citi stay safe everyon  \n",
       "2  forest fire spot pond gees flee across street ...  \n",
       "3                     apocalyps light spokan wildfir  \n",
       "4                 typhoon soudelor kill china taiwan  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4350f3-4909-4f4d-8279-6c22236fbae9",
   "metadata": {},
   "source": [
    "### Method1: Word Frequency\n",
    "\n",
    "To kick start a base method, let's start with using the count of words in each tweet.\n",
    "Below will be using `CountVectorizer` to build the count of words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "885d8e24-7fc1-4a0c-8e54-f649109231a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ee90c51-524d-4295-baca-489edd07d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d4a4a9c-c9ac-48b7-87aa-2a5db5d9c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deed reason earthquak may allah forgiv us'\n",
      " 'forest fire near la rong sask canada']\n",
      "['allah' 'canada' 'deed' 'earthquak' 'fire' 'forest' 'forgiv' 'la' 'may'\n",
      " 'near' 'reason' 'rong' 'sask' 'us']\n",
      "{'deed': 2, 'reason': 10, 'earthquak': 3, 'may': 8, 'allah': 0, 'forgiv': 6, 'us': 13, 'forest': 5, 'fire': 4, 'near': 9, 'la': 7, 'rong': 11, 'sask': 12, 'canada': 1}\n",
      "[[1 0 1 1 0 0 1 0 1 0 1 0 0 1]\n",
      " [0 1 0 0 1 1 0 1 0 1 0 1 1 0]]\n",
      "(2, 14)\n"
     ]
    }
   ],
   "source": [
    "## let's take a look at expected output by using first 2 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train[\"text_clean_string\"][0:2])\n",
    "print(train[\"text_clean_string\"][0:2].values)\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_vectorizer.vocabulary_)\n",
    "print(example_train_vectors.toarray())\n",
    "print(example_train_vectors.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49259b4-4628-460b-90ef-3f0b8b1dc0cb",
   "metadata": {},
   "source": [
    "The above tells us that, there are 14 unique words (or \"tokens\") in the first two tweets.\n",
    "\n",
    "Now let's create vectors for all of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b644b64a-29f3-4770-95af-dfeaccbc3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train[\"text_clean_string\"])\n",
    "\n",
    "# note that we're NOT using .fit_transform() here. Using just .transform() makes sure that train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test[\"text_clean_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fad47072-8736-44c8-95ca-324dbd19dc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 10532)\n"
     ]
    }
   ],
   "source": [
    "# print(count_vectorizer.get_feature_names_out())\n",
    "# print(count_vectorizer.vocabulary_)\n",
    "# print(example_train_vectors.toarray())\n",
    "print(train_vectors.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f0c7d-ed8c-4683-a9fe-68b04bbcc377",
   "metadata": {},
   "source": [
    "1-1 Build model without standerdize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "caaacc58-c042-42b6-8532-7800244d9478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9558550185873607\n",
      "F1 Score: 0.8038834951456311\n",
      "      0     1\n",
      "0  3843    42\n",
      "1   148  2057\n",
      "     0    1\n",
      "0  907   65\n",
      "1  137  414\n"
     ]
    }
   ],
   "source": [
    "X = train_vectors\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_train, LR.predict(X_train)))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, LR.predict(X_train))))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "443d9dfc-f009-4f3f-a0ef-0b7834748ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiroshima: 3.2235906811173214\n",
      "earthquak: 2.433729953999329\n",
      "wildfir: 2.35408079442359\n",
      "typhoon: 2.222483661423877\n",
      "storm: 2.0965145002339285\n",
      "hailstorm: 2.0797444679573003\n",
      "terror: 2.07040024737212\n",
      "massacr: 1.937770617140202\n",
      "flood: 1.8580075912108673\n",
      "debri: 1.8265627496019627\n",
      "----------------------------------------\n",
      "love: -1.6729591328635658\n",
      "obliter: -1.4964097901958806\n",
      "im: -1.4532273580240052\n",
      "play: -1.3728193747669386\n",
      "best: -1.362140773368952\n",
      "let: -1.2809787867933276\n",
      "poll: -1.2556801293626336\n",
      "write: -1.2239639356602108\n",
      "phone: -1.2122294344012605\n",
      "new: -1.2110189863305614\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words) from the CountVectorizer\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefficients = LR.coef_[0]\n",
    "\n",
    "# Create a dictionary that associates feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the features by their coefficients to find the most important positive coeficient ones\n",
    "sorted_features_positive = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort the features by their coefficients to find the most important negative coeficient ones\n",
    "sorted_features_negative = sorted(feature_coefficients.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the top N most important positive features\n",
    "top_n = 10  # Change this value to see more or fewer features\n",
    "for feature, coefficient in sorted_features_positive[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "print('----------------------------------------')\n",
    "    \n",
    "# Print the top N most important negative features\n",
    "for feature, coefficient in sorted_features_negative[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88823f92-5f17-43bb-82ab-4c95269d8237",
   "metadata": {},
   "source": [
    "1-2 Build model with MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21590015-02df-4d23-a26f-901f2f2c343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9492719586660403\n",
      "F1 Score: 0.8003972194637538\n",
      "      0     1\n",
      "0  3853    32\n",
      "1   184  2021\n",
      "     0    1\n",
      "0  919   53\n",
      "1  148  403\n"
     ]
    }
   ],
   "source": [
    "#use scalor\n",
    "scaler = MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_vectors)\n",
    "X_test_scaled = scaler.transform(test_vectors)\n",
    "\n",
    "\n",
    "X = X_train_scaled\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_train, LR.predict(X_train)))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, LR.predict(X_train))))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "430ed870-e965-4ec1-b128-ded88e64a0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiroshima: 3.649444017555073\n",
      "fire: 3.0221014541643365\n",
      "storm: 2.856181514370036\n",
      "wildfir: 2.6064582762320248\n",
      "flood: 2.58462355719907\n",
      "earthquak: 2.5585862748905437\n",
      "california: 2.50585799480792\n",
      "evacu: 2.4007609046382927\n",
      "train: 2.386053040154951\n",
      "suicid: 2.3753110881717667\n",
      "----------------------------------------\n",
      "love: -2.0071768987091567\n",
      "like: -1.8033211990826956\n",
      "obliter: -1.7475210443364972\n",
      "new: -1.610437434648507\n",
      "im: -1.5424435352718318\n",
      "let: -1.4223989483274389\n",
      "crush: -1.4205795325621478\n",
      "want: -1.4199524024136811\n",
      "play: -1.3894126350324298\n",
      "best: -1.3775459572256539\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words) from the CountVectorizer\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefficients = LR.coef_[0]\n",
    "\n",
    "# Create a dictionary that associates feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the features by their coefficients to find the most important positive coeficient ones\n",
    "sorted_features_positive = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort the features by their coefficients to find the most important negative coeficient ones\n",
    "sorted_features_negative = sorted(feature_coefficients.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the top N most important positive features\n",
    "top_n = 10  # Change this value to see more or fewer features\n",
    "for feature, coefficient in sorted_features_positive[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "print('----------------------------------------')\n",
    "    \n",
    "# Print the top N most important negative features\n",
    "for feature, coefficient in sorted_features_negative[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839b2d6-760e-4f56-8ab7-58bb749eacf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3846346-6905-4a03-b634-03e251d4bc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.9104477611940298\n",
      "Test F1 Score: 0.7902621722846441\n",
      "      0     1\n",
      "0  3754   131\n",
      "1   253  1952\n",
      "     0    1\n",
      "0  877   95\n",
      "1  129  422\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Multinomial Naive Bayes model\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "train_predicted = nb_classifier.predict(X_train)\n",
    "test_predicted = nb_classifier.predict(X_test)\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b022fe-ede0-4946-916c-d6afe936dca8",
   "metadata": {},
   "source": [
    "### Method2: TF-IDF\n",
    "\n",
    "Now, let's try TF-IDF by using `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ea6ab7a-2aa0-4db8-a810-82ca151b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67f0b7e2-c3bd-4086-a21f-34fff2ce2d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 1520)\n"
     ]
    }
   ],
   "source": [
    "# Only include >=10 occurrences\n",
    "# Have unigrams and bigrams\n",
    "vec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n",
    "\n",
    "text_vec = vec_text.fit_transform(train['text_clean_string'])\n",
    "text_vec_test = vec_text.transform(test['text_clean_string'])\n",
    "X_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names_out())\n",
    "X_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names_out())\n",
    "print (X_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3020362b-8c92-4d3b-9a88-c3e2608dac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablaz</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accid</th>\n",
       "      <th>accord</th>\n",
       "      <th>...</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth save</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282098</td>\n",
       "      <td>0.295891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 1520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aba  aba woman  abandon       abc  abc news  abl  ablaz  absolut  accid  \\\n",
       "0     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "1     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "2     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "3     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "4     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "...   ...        ...      ...       ...       ...  ...    ...      ...    ...   \n",
       "7608  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7609  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7610  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7611  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7612  0.0        0.0      0.0  0.282098  0.295891  0.0    0.0      0.0    0.0   \n",
       "\n",
       "      accord  ...  yesterday   yo  york  young  youth  youth save  youtub  \\\n",
       "0        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "1        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "2        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "3        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "4        0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "...      ...  ...        ...  ...   ...    ...    ...         ...     ...   \n",
       "7608     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7609     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7610     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7611     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "7612     0.0  ...        0.0  0.0   0.0    0.0    0.0         0.0     0.0   \n",
       "\n",
       "       yr  yyc  zone  \n",
       "0     0.0  0.0   0.0  \n",
       "1     0.0  0.0   0.0  \n",
       "2     0.0  0.0   0.0  \n",
       "3     0.0  0.0   0.0  \n",
       "4     0.0  0.0   0.0  \n",
       "...   ...  ...   ...  \n",
       "7608  0.0  0.0   0.0  \n",
       "7609  0.0  0.0   0.0  \n",
       "7610  0.0  0.0   0.0  \n",
       "7611  0.0  0.0   0.0  \n",
       "7612  0.0  0.0   0.0  \n",
       "\n",
       "[7613 rows x 1520 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cdc659e-5e11-48f9-b5cb-eabfb0d005ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8442703232125367\n",
      "F1 Score: 0.7745197168857432\n",
      "      0     1\n",
      "0  3730   155\n",
      "1   481  1724\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  917   55\n",
       "1  168  383"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train_text\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_train, LR.predict(X_train)))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, LR.predict(X_train))))\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "992ca6b9-e820-43de-bba2-1792308c03d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiroshima: 4.275421238239577\n",
      "wildfir: 3.3212316744158494\n",
      "kill: 3.0955466022564986\n",
      "california: 3.091339943710916\n",
      "earthquak: 3.0053209072353746\n",
      "flood: 2.6340086332177184\n",
      "build: 2.630763386661721\n",
      "storm: 2.5773261202259587\n",
      "evacu: 2.513961610048836\n",
      "terror: 2.5034294145736102\n",
      "----------------------------------------\n",
      "love: -2.5050454413254055\n",
      "new: -2.285640936987951\n",
      "obliter: -2.237746940088469\n",
      "like: -2.104307020737453\n",
      "want: -2.055299262699642\n",
      "let: -1.96135707284439\n",
      "play: -1.879522722322039\n",
      "crush: -1.832865425900002\n",
      "im: -1.8203578817790873\n",
      "scream: -1.7861272914707753\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (words) from the CountVectorizer\n",
    "feature_names = vec_text.get_feature_names_out()\n",
    "\n",
    "# Get the coefficients from the trained logistic regression model\n",
    "coefficients = LR.coef_[0]\n",
    "\n",
    "# Create a dictionary that associates feature names with their coefficients\n",
    "feature_coefficients = dict(zip(feature_names, coefficients))\n",
    "\n",
    "# Sort the features by their coefficients to find the most important positive coeficient ones\n",
    "sorted_features_positive = sorted(feature_coefficients.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Sort the features by their coefficients to find the most important negative coeficient ones\n",
    "sorted_features_negative = sorted(feature_coefficients.items(), key=lambda x: x[1])\n",
    "\n",
    "# Print the top N most important positive features\n",
    "top_n = 10  # Change this value to see more or fewer features\n",
    "for feature, coefficient in sorted_features_positive[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")\n",
    "\n",
    "print('----------------------------------------')\n",
    "    \n",
    "# Print the top N most important negative features\n",
    "for feature, coefficient in sorted_features_negative[:top_n]:\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e36b2a-dd4f-4e54-bad2-47176ebc6750",
   "metadata": {},
   "source": [
    "The TF-IDF result(0.766) is slightly lower than Word Frequency (0.77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d7bd08-48f6-4e4c-b761-91f6d2520723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0707d6b4-bda2-4e7f-b294-d84f62451a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.8135508155583437\n",
      "Test F1 Score: 0.7533818938605619\n",
      "      0     1\n",
      "0  3726   159\n",
      "1   584  1621\n",
      "     0    1\n",
      "0  924   48\n",
      "1  189  362\n"
     ]
    }
   ],
   "source": [
    "# Create and train the Multinomial Naive Bayes model\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train,y_train)\n",
    "train_predicted = nb_classifier.predict(X_train)\n",
    "test_predicted = nb_classifier.predict(X_test)\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0412fc-533b-414e-8dd9-a547f19f18cf",
   "metadata": {},
   "source": [
    "### Method3: Sentence Embedding\n",
    "\n",
    "Here, we're using `SentenceTransformer` to transform our text into sentence embedding.\n",
    "\n",
    "*Reference link: https://www.youtube.com/watch?v=c7AqnswslWo\n",
    "\n",
    "An experiment is performed to determine whether it is more effective to create sentence embeddings from the original text or from text that has undergone lowercase conversion, removal of non-alphabetic characters, exclusion of stop words, and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4df70d52-1a60-4319-8961-2b24fda483a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package\n",
    "# !pip install --user sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aaff51ed-5983-40ad-aa9c-651241675cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a87395-c473-4390-8e85-4fbc4be0c301",
   "metadata": {},
   "source": [
    "1. Embedding from processed text (lowercase, alphabetic, stop words, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db6996c0-83b5-4a8e-8e36-a05f1143e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SentenceTransformer to generate sentence embedding\n",
    "# can choose diff models from: https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train['embeddings'] = train['text_clean_string'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4426c7c2-88d8-4817-bb77-95005bfa0007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.771544521365481\n",
      "Test F1 Score: 0.7568093385214009\n",
      "      0     1\n",
      "0  3517   368\n",
      "1   589  1616\n",
      "     0    1\n",
      "0  884   88\n",
      "1  162  389\n"
     ]
    }
   ],
   "source": [
    "X = train['embeddings'].to_list()\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "train_predicted = LR.predict(X_train)\n",
    "test_predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e5ed7-bf48-4dd9-9efd-6097f9813b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b18a8985-3283-4a9d-a697-58165d55a728",
   "metadata": {},
   "source": [
    "2. Embedding from original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bbf24d00-eb47-4042-ad0f-2e4e369e0160",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_llm = pp.process_text(train_df,LLM = True)\n",
    "test_llm = pp.process_text(test_df,LLM = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cef96ee2-1cf4-4437-8b62-df8689208177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive wildfires evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...  \n",
       "1       1             Forest fire near La Ronge Sask. Canada  \n",
       "2       1  All residents asked to 'shelter in place' are ...  \n",
       "3       1  13,000 people receive wildfires evacuation ord...  \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aae004ca-afc2-4e68-97f4-725483bd7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SentenceTransformer to generate sentence embedding\n",
    "# can choose diff models from: https://www.sbert.net/docs/pretrained_models.html\n",
    "train_llm['embeddings'] = train_llm['text_clean'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "110c7eb4-0797-4540-96bd-8fc6036a7caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>[-0.0054457616, 0.064116865, 0.11215522, 0.033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[0.04022922, 0.03801415, -0.0064313184, 0.0246...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[0.13137569, 0.012401222, 0.06718611, 0.085462...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive wildfires evacuation ord...</td>\n",
       "      <td>[0.099470675, -0.033589236, 0.0065593175, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>[-0.036732815, 0.10448628, 0.0742257, 0.089533...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1       1             Forest fire near La Ronge Sask. Canada   \n",
       "2       1  All residents asked to 'shelter in place' are ...   \n",
       "3       1  13,000 people receive wildfires evacuation ord...   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.0054457616, 0.064116865, 0.11215522, 0.033...  \n",
       "1  [0.04022922, 0.03801415, -0.0064313184, 0.0246...  \n",
       "2  [0.13137569, 0.012401222, 0.06718611, 0.085462...  \n",
       "3  [0.099470675, -0.033589236, 0.0065593175, 0.01...  \n",
       "4  [-0.036732815, 0.10448628, 0.0742257, 0.089533...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf1497ba-e72c-4332-9640-d8673a70bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.8250116658889407\n",
      "Test F1 Score: 0.787878787878788\n",
      "      0     1\n",
      "0  3572   313\n",
      "1   437  1768\n",
      "     0    1\n",
      "0  883   89\n",
      "1  135  416\n"
     ]
    }
   ],
   "source": [
    "X = train_llm['embeddings'].to_list()\n",
    "y = train_llm['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "train_predicted = LR.predict(X_train)\n",
    "test_predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa23b8e-8829-4119-90f8-acefb28dfd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745762cf-5bc6-4b15-847d-c053d9e9e2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5f1a4c-7581-44a0-b50f-c901166dc4b0",
   "metadata": {},
   "source": [
    "If you run the code below, you will get the error: Negative values in data passed to MultinomialNB (input X)\n",
    "\n",
    "This is because MultinomialNB (Multinomial Naive Bayes) algorithm is designed for discrete data, specifically for data that represents counts or frequencies. In the context of text classification, it's commonly used to model the frequency of words in a document or a set of documents.\n",
    "\n",
    "Negative values are not meaningful in this context because you cannot have a negative count of words in a document. The Multinomial Naive Bayes algorithm works with non-negative integer values, which typically represent the frequency or count of each term (word) in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "62d17eb2-e75e-44d2-b519-fb4451a28012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and train the Multinomial Naive Bayes model\n",
    "# nb_classifier = MultinomialNB()\n",
    "# nb_classifier.fit(X_train,y_train)\n",
    "# train_predicted = nb_classifier.predict(X_train)\n",
    "# test_predicted = nb_classifier.predict(X_test)\n",
    "# print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "# print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# # Confusion matrix\n",
    "# print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "# print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bdcd908d-8618-4584-9b59-4bb34a1ad674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.8316600513658651\n",
      "Test F1 Score: 0.7823585810162992\n",
      "      0     1\n",
      "0  3588   297\n",
      "1   424  1781\n",
      "     0    1\n",
      "0  888   84\n",
      "1  143  408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# X_train and y_train are your sentence embeddings and labels\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "train_predicted = svm_classifier.predict(X_train)\n",
    "test_predicted = svm_classifier.predict(X_test)\n",
    "print(\"Train F1 Score:\",f1_score(y_train, train_predicted))\n",
    "print(\"Test F1 Score:\",f1_score(y_test, test_predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, train_predicted)))\n",
    "print(pd.DataFrame(confusion_matrix(y_test, test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb48d6-475a-4025-8845-4869f0fe5773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
