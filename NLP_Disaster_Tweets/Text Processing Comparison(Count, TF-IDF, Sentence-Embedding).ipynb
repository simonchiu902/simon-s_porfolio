{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a054eb8-cf6e-401d-b731-b084e35bcbaf",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This notebook aims to explore the impact of various text processing techniques on prediction outcomes. Specifically, it will investigate the performance differences between methods such as word frequency analysis, TF-IDF (Term Frequency-Inverse Document Frequency), and sentence embedding in the context of making predictions.\n",
    "\n",
    "To ensure that variations primarily arise from different text processing techniques, we will exclusively employ logistic regression as our chosen classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96110f4d-821e-487a-8114-54d6d1627a35",
   "metadata": {},
   "source": [
    "### Import Basic Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dfb626-7e21-4bf9-a8f9-8f7ed08319ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a50e6c8-9133-4577-91d7-ce9a38ef6c59",
   "metadata": {},
   "source": [
    "### Import Packages for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e08a2fc8-62fb-40b2-bebd-07c416daf7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, f1_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88f867-901c-4988-9c0a-e80a0dbff173",
   "metadata": {},
   "source": [
    "### Read Files\n",
    "\n",
    "The dataset is downloaded from https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c30d1eaa-01ff-4898-8b2f-904c2e4f7a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"./kaggle/input/train.csv\")\n",
    "test_df = pd.read_csv(\"./kaggle/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ba9620-ce2f-40c7-ac63-d1f775b32226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac2f65e-2afa-4d62-897f-2af161f889a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456a480-33d8-44bd-b09b-065d00c8085a",
   "metadata": {},
   "source": [
    "### Clean Text Column\n",
    "\n",
    "Text cleaning is done from a python script (Preprocessing_for_Text-Processing-Comparison.py). The following steps are involved:\n",
    "\n",
    "1. remove link\n",
    "2. remove line breaks\n",
    "3. remove # from hashtag\n",
    "4. tokenize text\n",
    "5. change words into lower case\n",
    "6. only include alphabetic words\n",
    "7. remove stop words\n",
    "8. steeming words\n",
    "\n",
    "Upon completion of the cleaning process, two additional columns are created. The first column, named `text_clean`, consists of a list of tokenized words. The second column, denoted as `text_clean_string`, is a representation formed by combining the tokens from the text_clean column with spaces in between each token. They will be used for the three different methods later.\n",
    "\n",
    "For more details, can refer to the python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56e22e2-6346-4dd1-a083-440c95fb3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preprocessing_for_Text_Processing_Comparison as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098e8ea8-7be8-42a8-aaf8-35069dac5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pp.process_text(train_df)\n",
    "test = pp.process_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ab90f5-d2ef-41d1-8434-95fb6b1900b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, place, notifi, offic, evacu, shel...</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2       1  [resid, ask, place, notifi, offic, evacu, shel...   \n",
       "3       1  [peopl, receiv, wildfir, evacu, order, califor...   \n",
       "4       1  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   text_clean_string  \n",
       "0          deed reason earthquak may allah forgiv us  \n",
       "1               forest fire near la rong sask canada  \n",
       "2  resid ask place notifi offic evacu shelter pla...  \n",
       "3        peopl receiv wildfir evacu order california  \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fd723b-4fd2-4953-b18f-495c14b0ff31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>[happen, terribl, car, crash]</td>\n",
       "      <td>happen terribl car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>[heard, earthquak, differ, citi, stay, safe, e...</td>\n",
       "      <td>heard earthquak differ citi stay safe everyon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>[forest, fire, spot, pond, gees, flee, across,...</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>[apocalyps, light, spokan, wildfir]</td>\n",
       "      <td>apocalyps light spokan wildfir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                      [happen, terribl, car, crash]   \n",
       "1  [heard, earthquak, differ, citi, stay, safe, e...   \n",
       "2  [forest, fire, spot, pond, gees, flee, across,...   \n",
       "3                [apocalyps, light, spokan, wildfir]   \n",
       "4           [typhoon, soudelor, kill, china, taiwan]   \n",
       "\n",
       "                                   text_clean_string  \n",
       "0                           happen terribl car crash  \n",
       "1      heard earthquak differ citi stay safe everyon  \n",
       "2  forest fire spot pond gees flee across street ...  \n",
       "3                     apocalyps light spokan wildfir  \n",
       "4                 typhoon soudelor kill china taiwan  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4350f3-4909-4f4d-8279-6c22236fbae9",
   "metadata": {},
   "source": [
    "### Method1: Word Frequency\n",
    "\n",
    "To kick start a base method, let's start with using the count of words in each tweet.\n",
    "Below will be using `CountVectorizer` to build the count of words matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885d8e24-7fc1-4a0c-8e54-f649109231a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee90c51-524d-4295-baca-489edd07d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4a4a9c-c9ac-48b7-87aa-2a5db5d9c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deed reason earthquak may allah forgiv us'\n",
      " 'forest fire near la rong sask canada']\n",
      "['allah' 'canada' 'deed' 'earthquak' 'fire' 'forest' 'forgiv' 'la' 'may'\n",
      " 'near' 'reason' 'rong' 'sask' 'us']\n",
      "{'deed': 2, 'reason': 10, 'earthquak': 3, 'may': 8, 'allah': 0, 'forgiv': 6, 'us': 13, 'forest': 5, 'fire': 4, 'near': 9, 'la': 7, 'rong': 11, 'sask': 12, 'canada': 1}\n",
      "[[1 0 1 1 0 0 1 0 1 0 1 0 0 1]\n",
      " [0 1 0 0 1 1 0 1 0 1 0 1 1 0]]\n",
      "(2, 14)\n"
     ]
    }
   ],
   "source": [
    "## let's take a look at expected output by using first 2 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train[\"text_clean_string\"][0:2])\n",
    "print(train[\"text_clean_string\"][0:2].values)\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "print(count_vectorizer.vocabulary_)\n",
    "print(example_train_vectors.toarray())\n",
    "print(example_train_vectors.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49259b4-4628-460b-90ef-3f0b8b1dc0cb",
   "metadata": {},
   "source": [
    "The above tells us that, there are 14 unique words (or \"tokens\") in the first two tweets.\n",
    "\n",
    "Now let's create vectors for all of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b644b64a-29f3-4770-95af-dfeaccbc3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text_clean_string\"])\n",
    "\n",
    "# note that we're NOT using .fit_transform() here. Using just .transform() makes sure that train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text_clean_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fad47072-8736-44c8-95ca-324dbd19dc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 11974)\n"
     ]
    }
   ],
   "source": [
    "# print(count_vectorizer.get_feature_names_out())\n",
    "# print(count_vectorizer.vocabulary_)\n",
    "# print(example_train_vectors.toarray())\n",
    "print(train_vectors.toarray().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5328d99e-fc93-441c-b249-86f737357e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7745571658615136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>762</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  762  107\n",
       "1  173  481"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_vectors\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b022fe-ede0-4946-916c-d6afe936dca8",
   "metadata": {},
   "source": [
    "### Method2: TF-IDF\n",
    "\n",
    "Now, let's try TF-IDF by using `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ea6ab7a-2aa0-4db8-a810-82ca151b7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67f0b7e2-c3bd-4086-a21f-34fff2ce2d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 1520)\n"
     ]
    }
   ],
   "source": [
    "# Only include >=10 occurrences\n",
    "# Have unigrams and bigrams\n",
    "vec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n",
    "\n",
    "text_vec = vec_text.fit_transform(train['text_clean_string'])\n",
    "text_vec_test = vec_text.transform(test['text_clean_string'])\n",
    "X_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names_out())\n",
    "X_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names_out())\n",
    "print (X_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3020362b-8c92-4d3b-9a88-c3e2608dac62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aba</th>\n",
       "      <th>aba woman</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abl</th>\n",
       "      <th>ablaz</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accid</th>\n",
       "      <th>accord</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth save</th>\n",
       "      <th>youtub</th>\n",
       "      <th>youtub playlist</th>\n",
       "      <th>youtub video</th>\n",
       "      <th>yr</th>\n",
       "      <th>yyc</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.279019</td>\n",
       "      <td>0.296052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 1520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aba  aba woman  abandon       abc  abc news  abl  ablaz  absolut  accid  \\\n",
       "0     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "1     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "2     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "3     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "4     0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "...   ...        ...      ...       ...       ...  ...    ...      ...    ...   \n",
       "7608  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7609  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7610  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7611  0.0        0.0      0.0  0.000000  0.000000  0.0    0.0      0.0    0.0   \n",
       "7612  0.0        0.0      0.0  0.279019  0.296052  0.0    0.0      0.0    0.0   \n",
       "\n",
       "      accord  ...  york  young  youth  youth save  youtub  youtub playlist  \\\n",
       "0        0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "1        0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "2        0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "3        0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "4        0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "...      ...  ...   ...    ...    ...         ...     ...              ...   \n",
       "7608     0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "7609     0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "7610     0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "7611     0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "7612     0.0  ...   0.0    0.0    0.0         0.0     0.0              0.0   \n",
       "\n",
       "      youtub video   yr  yyc  zone  \n",
       "0              0.0  0.0  0.0   0.0  \n",
       "1              0.0  0.0  0.0   0.0  \n",
       "2              0.0  0.0  0.0   0.0  \n",
       "3              0.0  0.0  0.0   0.0  \n",
       "4              0.0  0.0  0.0   0.0  \n",
       "...            ...  ...  ...   ...  \n",
       "7608           0.0  0.0  0.0   0.0  \n",
       "7609           0.0  0.0  0.0   0.0  \n",
       "7610           0.0  0.0  0.0   0.0  \n",
       "7611           0.0  0.0  0.0   0.0  \n",
       "7612           0.0  0.0  0.0   0.0  \n",
       "\n",
       "[7613 rows x 1520 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cdc659e-5e11-48f9-b5cb-eabfb0d005ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7600989282769992\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>771</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>193</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  771   98\n",
       "1  193  461"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train_text\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e36b2a-dd4f-4e54-bad2-47176ebc6750",
   "metadata": {},
   "source": [
    "The TF-IDF result is slightly lower than Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0412fc-533b-414e-8dd9-a547f19f18cf",
   "metadata": {},
   "source": [
    "### Method3: Sentence Embedding\n",
    "\n",
    "Here, we're using `SentenceTransformer` to transform our text into sentence embedding.\n",
    "\n",
    "*Reference link: https://www.youtube.com/watch?v=c7AqnswslWo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df70d52-1a60-4319-8961-2b24fda483a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install package\n",
    "# !pip install --user sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aaff51ed-5983-40ad-aa9c-651241675cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db6996c0-83b5-4a8e-8e36-a05f1143e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SentenceTransformer to generate sentence embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "train['embeddings'] = train['text_clean_string'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b64f41f-6976-4cda-92ec-7629c6c97b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>[-0.033004314, 0.1294759, 0.027240366, -0.0689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>[0.036669217, 0.02750025, 0.01003717, 0.015032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[resid, ask, place, notifi, offic, evacu, shel...</td>\n",
       "      <td>resid ask place notifi offic evacu shelter pla...</td>\n",
       "      <td>[-0.0040206, 0.11070472, 0.02508261, 0.0450453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>[0.036597367, 0.06827786, 0.023573978, 0.01135...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>[-0.081632115, 0.077032514, 0.03796497, 0.0621...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1       1       [forest, fire, near, la, rong, sask, canada]   \n",
       "2       1  [resid, ask, place, notifi, offic, evacu, shel...   \n",
       "3       1  [peopl, receiv, wildfir, evacu, order, califor...   \n",
       "4       1  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "                                   text_clean_string  \\\n",
       "0          deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  resid ask place notifi offic evacu shelter pla...   \n",
       "3        peopl receiv wildfir evacu order california   \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [-0.033004314, 0.1294759, 0.027240366, -0.0689...  \n",
       "1  [0.036669217, 0.02750025, 0.01003717, 0.015032...  \n",
       "2  [-0.0040206, 0.11070472, 0.02508261, 0.0450453...  \n",
       "3  [0.036597367, 0.06827786, 0.023573978, 0.01135...  \n",
       "4  [-0.081632115, 0.077032514, 0.03796497, 0.0621...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d693d02-f831-485b-863d-a19b16990e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7488076311605724\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>736</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>183</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  736  133\n",
       "1  183  471"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train['embeddings'].to_list()\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646f0d32-7f93-4463-b501-c13175568387",
   "metadata": {},
   "source": [
    "The sentence embedding F1 score is the worst among three methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0fecd-6d85-4f38-9d43-03c2bcc4d3f5",
   "metadata": {},
   "source": [
    "### Conlcusion\n",
    "\n",
    "Final F1 Score: \n",
    "* Word-Frequency: 0.77\n",
    "* TF-IDF: 0.76\n",
    "* Sentence-Embedding: 0.74\n",
    "\n",
    "The result seems counter-intuitive, this may be caused by data preprocessing, finetunning, the choice of classification model, and the selection of embedding models.\n",
    "\n",
    "Future efforts will delve into these potential areas of concern with the aim of enhancing them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
