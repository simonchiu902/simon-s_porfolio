{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9458136-7e16-4f8a-b845-cd5f03d9fe93",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "From Text Processing Comparison notebook, we already can get around 0.7 F1 score by logistic regression. Moreover, we can use `get_feature_names_out` and `LogisticRegression.coef_` to find some important words in tweets that indicate disaster or non-disaster tweets. \n",
    "\n",
    "This file will further look into those false positive and false negative records to see if we can do better preprocessing for the dataset to make better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfce90-f85c-4133-9ccb-b462de341315",
   "metadata": {},
   "source": [
    "### Import Basic Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b75370-c75c-4404-a7cc-13b7f838df7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cef0af-5de1-46dd-a09e-3da43ea5f8c5",
   "metadata": {},
   "source": [
    "### Import Packages for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8561dfa-0fda-436f-b9b8-51e20e112ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, f1_score,confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227760b-0605-4f6d-9169-0df734743806",
   "metadata": {},
   "source": [
    "### Read Files and do prediction\n",
    "\n",
    "The dataset is downloaded from https://www.kaggle.com/c/nlp-getting-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0efba82-d0d7-4588-92e5-73bde37f397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(\"./kaggle/input/train.csv\")\n",
    "test_df = pd.read_csv(\"./kaggle/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d2614a2-aa57-4f95-aa6d-453144856283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preprocessing_for_Text_Processing_Comparison as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0680f951-9233-4fb3-bf09-3a0dcaf617b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pp.process_text(train_df)\n",
    "test = pp.process_text(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76862760-ae61-4a1c-b422-3486e468b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcb8cd30-a5ba-49fc-812c-56246601c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 1511)\n"
     ]
    }
   ],
   "source": [
    "# Only include >=10 occurrences\n",
    "# Have unigrams and bigrams\n",
    "vec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n",
    "\n",
    "text_vec = vec_text.fit_transform(train['text_clean_string'])\n",
    "text_vec_test = vec_text.transform(test['text_clean_string'])\n",
    "X_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names_out())\n",
    "X_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names_out())\n",
    "print (X_train_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e5ea351-cd1e-450b-b05e-2de23f4f098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter out the FutureWarning related to is_sparse\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"sklearn.utils.validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38099a7c-0c05-402b-a52f-81131292eee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7977644380045539\n",
      "F1 Score: 0.763396537510305\n",
      "      0     1\n",
      "0  3186   287\n",
      "1   690  1927\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>773</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>191</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  773   96\n",
       "1  191  463"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train_text\n",
    "y = train['target'].to_list()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "predicted_test = LR.predict(X_test)\n",
    "# print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "# print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"F1 Score:\",f1_score(y_train, LR.predict(X_train)))\n",
    "print(\"F1 Score:\",f1_score(y_test, predicted))\n",
    "# Confusion matrix\n",
    "print(pd.DataFrame(confusion_matrix(y_train, LR.predict(X_train))))\n",
    "pd.DataFrame(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e9e6f-59ef-4a69-bf90-45d68a073047",
   "metadata": {},
   "source": [
    "### Find False Positive and False Negative Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54d45b39-84bc-4c52-8dde-8a5bf7041822",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = LR.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1e68d00-b604-4d87-a8d2-20a037f28a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find false positive records in training data\n",
    "prediction_train = LR.predict(X_train)\n",
    "FP_indices_train = [X_train.iloc[i].name for i in range(len(X_train)) if y_train[i] == 0 and prediction_train[i] == 1]\n",
    "FP_train = train.loc[FP_indices_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc8a1dff-d374-48dd-b8b6-020cb9c7cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#easier to see full text in excel\n",
    "FP_train.to_csv('FP_train.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b8b009-3261-48df-a4e9-5f6a77a661b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find False Negative (FN) records in the test data\n",
    "FN_indices_test = [X_test.iloc[i].name for i in range(len(X_test)) if y_test[i] == 1 and predicted_test[i] == 0]\n",
    "FN_test = train.loc[FN_indices_test]\n",
    "#Find false negative records in training data\n",
    "FN_indices_train = [X_train.iloc[i].name for i in range(len(X_train)) if y_train[i] == 1 and prediction_train[i] == 0]\n",
    "FN_train = train.loc[FN_indices_train]\n",
    "# combine FN records in both train and test data\n",
    "full_FN = pd.concat([FN_test, FN_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e91faf9a-a000-432f-bcf0-ea8f58415f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#easier to see full text in excel\n",
    "full_FN.to_csv('full_FN.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3acbb46-ce47-476b-9170-b922ffc06f4b",
   "metadata": {},
   "source": [
    "### Demonstration of FP\n",
    "\n",
    "The tweet contains <b>building burning</b>, which may looks like a disaster, but actually it's just a <b>metaphor</b>.\n",
    "    \n",
    "This is ChatGPT's explanation for this sentence:\n",
    "1. \"I'm mentally preparing myself for a bomb ass school year\": This part suggests that the speaker is getting ready or mentally bracing themselves for an exceptionally good or exciting school year. \"Bomb ass\" is a slang term that means something is really excellent or outstanding.\n",
    "\n",
    "2. \"if it's not\": This part introduces a condition or possibility. The speaker is implying that they are excited and prepared for a great school year, but there's also a chance it might not turn out that way.\n",
    "\n",
    "3. \"I'm burning buildings\": This phrase can be metaphorical and hyperbolic. It doesn't literally mean setting fire to buildings. Instead, it may express the speaker's frustration or anger. In this context, it suggests that if the school year doesn't meet their expectations, they will feel upset or let down.\n",
    "\n",
    "\n",
    "Only using TF-IDF or other methods that doesn't take ambiguous meanings into consideration may cause this kind of problem.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10bab496-bed7-49f8-b893-47283ef5969b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm mentally preparing myself for a bomb ass school year if it's not I'm burning buildings ??\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FP_train.loc[1204]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14db31-a854-45fd-a4b1-2e5f735925b6",
   "metadata": {},
   "source": [
    "### Demonstration of FN\n",
    "\n",
    "Some records are labelled as disaster in the dataset, but they looks like not disaster-related tweets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49ff906d-d8da-40ca-894a-eaf0030292eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My back is so sunburned :('"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_FN.loc[4509]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11891e1c-6f94-43f7-baf7-9275d7ab6d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I went to pick up my lunch today and the bartender was holding my change hostage because he wanted my number. ??'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_FN.loc[4444]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a7f1d6-415f-4aeb-b9ba-e8070f3702e0",
   "metadata": {},
   "source": [
    "To decide wether relabelling the dataset, we look into the whole dataset and find that there are about 12% mislabelled records out of 150 records and most of them are mislabelled as 1. \n",
    "With a data-centric mindset and the goal of ensuring that the model can better serve our use case, which is to assist the government in detecting emergent tweets caused by disasters, we have decided to relabel the false negative (FN) data. (While it would be ideal to relabel the entire dataset, we didn't do so due to time and resource constraints.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b23b7af-8f13-4a94-8fcc-16416112e6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
